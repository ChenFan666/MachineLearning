# Bagging和随机森林
- 前言
	- 集成学习是目前机器学习的一大热门方向。简单来说，集成学习就是组合许多弱模型以得到一个预测结果比较好的强模型。对于常见的分类问题就是指采用多个分类器对数据集进行预测，把这些分类器的分类结果进行某种组合（如投票）决定分类结果，从而整体提高分类器的泛化能力。
	- 集成学习对于**大数据集和不充分数据**都有很好的效果。因为一些简单模型数据量太大而很难训练，或者只能学习到一部分，而集成学习方法可以有策略地将数据集划分成一些小数据集，并分别进行训练，之后根据一些策略进行组合。相反，如果数据量很少，可以使用bootstrap进行抽样，得到多个数据集，分别进行训练后再组合。
	- 集成学习中组合的模型可以是同一类型的模型，也可以是不同类型的模型。根据采用的数据采样、预测方法等的不同，常见的集合组合策略主要有平均算法和Boosting两类。其中，平均算法利用不同估计算法的结果平均进行预测，在估计模型上按照不同的变化形式可以进一步划分为粘合（Pasting）、分袋（Bagging）、子空间（Subspacing）和分片（Patches）等。Boosting算法通过一系列聚合的估计模型加权平均进行预测。
	- 其中比较典型的算法就是随机森林方法和AdaBoost方法。
- 简介