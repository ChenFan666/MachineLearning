# 决策树
### 地位
  - 无论是在机器学习，深度学习，数据挖掘领域都有巨大的存在价值。
### 含义
  - 顾名思义，用于做出决策选择（分类）的树状模型。
### 组成
  - 常规而言，一般由判断模块，分支，终止模块组成。
  - 理解上，可以认为所谓的决策树就是一个多层次的if-else组合，一条走到终止模块的路径就是一个分类选择。
  - 但是路径上的if-else是互斥且完备的，这也就是说每一个实例都被一条路径且只被一条路径覆盖。
### 使用过程
  - 收集并且整理数据
  - 分析数据
  - 训练算法（构造一个决策树的数据结构）
  - 测试算法（使用经验树测试错误率，在可接受范围内，我们说决策树可用）
  - 使用算法（适用于所有监督学习算法，决策树便于理解数据内在联系）
### 决策树构建的预备事项
  - 常见问题：
	  - 数据收集不合理（各种原因），特征不足以构造完备的决策树
	  - 数据收集过于冗余，不能确定选取什么特征使用。
	  - 构建第一步
		  - 特征选择（选取对训练数据具有分类能力的特征）
			  - 一般的选择标准是信息增益最高的特征（需要概率论基础)
				  - 这里你必须先会计算香农熵
				  - 计算公式为
				  - ![公式1](http://g.recordit.co/z8wxCP2RUe.gif)
				  - 进一步能够计算条件香农熵
				  - 计算公式为
				  - ![公式2](http://g.recordit.co/z5rUIUtFjN.gif)
				  - 到这里，基本上就能够计算信息增益了
				  - 计算公式为
				  - ![公式3](http://g.recordit.co/XVBsWEhphn.gif)
				  - 有过概率论基础的，其实已经知道我在说什么了，这些其实都不难求。
				  - - 我已经写了参考脚本（模块化可以直接调用，传入ndarray类型的矩阵即可）
		  - 决策树生成和修剪
			  - 很多算法，个人常用CART和ID3
			   - 这里稍微提一下ID3算法：ID3算法的核心是在决策树各个结点上使用信息增益准则选择特征，递归地构建决策树。
			    具体实现方法是：从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子节点（一般是个多叉树）；同样的，对子结点递归地调用以上方法，构建决策树。递归直到所有特征的信息增益均很小或没有特征可以选择为止，最后得到一个决策树。
			  - 考虑处理过拟合问题
### 决策树构建
  - 一般递归构建
  - sklearn创建决策树
	  - 主要使用两个包
	  - DecisionTreeClassifier和export_graphviz
	  - 前者用于创建决策树，后者用于决策树可视化
	  - 具体使用可以参考官方文档，封装好的东西使用不难。

### 决策树的一些缺陷
- 树可能相当复杂
- 不能很好的预测，过拟合
- 可能不稳定，小的变动会让树的结构完全不同
- 不均衡化的数据可能树极度偏差，耗费存储空间



