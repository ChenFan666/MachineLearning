# 朴素贝叶斯（NB）
- 简介
	- 一个简单的概率分类器。
	- 朴素贝叶斯分类器（Naive Bayes Classifier，NBC）发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。
	- 同时NBC模型所需要估计的参数很少，对缺失数据不太敏感，算法比较简单。之所以称为“朴素”是因为整个形式化过程只做最简单、最原始的假设。
	- 朴素贝叶斯在数据较少的情况下仍然有效，可以处理多分类问题。
- 原理
	- 基本公式
		- ![](https://img-blog.csdnimg.cn/20190124175014400.png)
	- 换一种表达方式
		- ![](https://img-blog.csdnimg.cn/20190124175200418.png)
		- 这看起来没有什么区别，但是提出来了几个概念。
			- P(B)称为“先验概率”（Prior probability），即在B事件发生之前，对A事件概率进行判断。
			- P(B|A)称为“后验概率”（Posterior probability），即在B事件发生之后，对A事件概率的重新判断。
			- P(A|B)/P(A)称为“可能性函数”（Likelyhood），这是一个调整因子，使得预估概率更接近真实概率。
			- 也就是说 后验概率 = 先验概率 * 调整因子
			- 这就是贝叶斯推断，其含义就是先估计一个“先验概率”，然后加入实验数据，观察这个数据是加强还是减弱了“先验概率”，从而得到更加接近事实的“后验概率”。
			- 若调整因子大于1表示加强“先验概率”，反之减弱，等于1表示没有影响。
			- 朴素贝叶斯则和贝叶斯推断有所不同，它认为条件概率分布的条件是独立不相关的。（在概率论里就是可不可拆分概率的区别）
				-这就带来了一些问题。首先，既然计算方便，那么概率连乘就被允许了，然而这是有很大的风险的，因为只要一个概率为0，那么结果毋庸置疑就是0了，这里可以使用拉普拉斯平滑处理（加1平滑）。其次，小数相乘只会越来越小，在近似运算后可能出现不可预计的后果（如近似得0风险），这就是下溢出问题，解决方法一般是取对数。
	- 也就是说，如果事先知道了P(特征)、P(类别)、P(特征|类别)这些数据，就可以求出P(类别|特征)了。
	- 下面看一个简单案例。
		- 下表给出了反映一个男生条件的基本数据，以及依据这些数据得到的女生是嫁还是不嫁的结果。

			| 外貌 | 性格 | 身高 | 上进心 | 嫁否 |
			| :---:|:---:|:---:|:---:|:---:|
			| 帅(A1) | 不好(B1) | 矮(C1) | 不上进(D1)| 不嫁(F)|
			| 帅(A1) | 不好(B1) | 矮(C1) | 上进(D2)| 不嫁(F)|
			| 帅(A1) | 好(B2) | 矮(C1) | 上进(D2)| 嫁(T)|
			| 帅(A1) | 好(B2) | 高(C2) | 不上进(D1)| 嫁(T)|
			| 帅(A1) | 好(B2) | 中(C3) | 上进(D2)| 嫁(T)|
			| 帅(A1) | 好(B2) | 矮(C1) | 不上进(D1)| 不嫁(F)|
			| 不帅(A2) | 好(B2) | 中(C3) | 上进(D2)| 嫁(T)|
			| 不帅(A2) | 好(B2) | 矮(C1) | 上进(D2)| 不嫁(F)|
			| 不帅(A2) | 好(B2) | 高(C2) | 上进(D2)| 嫁(T)|
			| 不帅(A2) | 不好(B1) | 高(C2) | 上进(D2)| 嫁(T)|
		- 如果一个男生向女生求婚，男生的特点是不帅、性格不好、矮、不上进，判断一下女生是嫁还是不嫁？（我觉得是个人都不会嫁）
		- 这个分类问题转化为数学问题就是P(嫁|(不帅、不好、矮、不上进))与P(不嫁|(不帅、不好、矮、不上进))谁的概率大。
		- 根据朴素贝叶斯公式
			- P(T|A2,B1,C1,D1)=P(A2,B1,C1,D1|T)*P(T)/P(A2,B1,C1,D1)
		- 现在只需要分别求出P(A2,B1,C1,D1|T)、P(A2,B1,C1,D1)、P(T)的概率即可。因为特征之间相互独立，所以有
			- P(A2,B1,C1,D1|T)=P(A2|T)*P(B1|T)*P(C1|T)*P(D1|T)
			- 代入原来的第一个求式可得
			- P(T|A2,B1,C1,D1)=P(A2|T)*P(B1|T)*P(C1|T)*P(D1|T)*P(T)/P(A2)*P(B1)*P(C1)*P(D1)
			- 首先统计“嫁”的概率，即P(T)的概率：
				- P(T)=嫁的数量/样本总数=6/122=1/2
			- 接着计算P(不帅|嫁)的概率，如下表

				| 外貌 | 性格 | 身高 | 上进心 | 嫁否 |
				| :---:|:---:|:---:|:---:|:---:|
				| 不帅(A2) | 好(B2) | 高(C2) | 上进(D2)| 嫁(T)|
				| 不帅(A2) | 好(B2) | 中(C3) | 上进(D2)| 嫁(T)|
				| 不帅(A2) | 不好(B1) | 高(C2) | 上进(D2)| 嫁(T)|
			- 由上表得出
				- P(A2|T)=3/6=1/2
			- 同理
				- P(性格不好|嫁)=P(B1|T)=1/6
				- P(矮|嫁)=P(C1|T)=1/6
				- P(不上进|嫁)=P(D1|T)=1/6
				- P(不帅)=P(A2)=4//12=1/3
				- P(性格不好)=P(B1)=4/12=1/3
				- P(矮)=P(C1)=7/12
				- P(不上进)=P(D1)=4/12=1/3
			- 最终计算得到
				- (T|A2,B1,C1,D1)=(1/864)/(7/324)
			- 同样的方法求出P(不嫁|(不帅、不好、矮、不上进))
				- P(F|A2,B1,C1,D1)=(1/48)/(7/324)
			- 很明显1/48>1/864于是P(F|A2,B1,C1,D1)>P(T|A2,B1,C1,D1)
			- 所以，根据朴素贝叶斯可以得出答案是不嫁。
- 优缺点
	- 朴素贝叶斯优点
		- 它是生成式模型，通过计算概率来进行分类，所以它可以用来处理多分类问题。
		- 算法简单易懂，数据少时表现好。
	- 朴素贝叶斯缺点
		- 对输入数据格式要求高。
		- 准确率丢失。（由于朴素假设）
		- 需要计算先验概率，分类决策存在错误率。
		- 朴素贝叶斯的准确率，是比较依赖于训练集的，其实不只是它，很多机器学习的算法的可能性都取决于训练条件。
- 实战
	- 使用朴素贝叶斯过滤垃圾邮件
	- 说明
		- 这是朴素贝叶斯最著名的应用：电子邮件垃圾过滤。
		- 流程如下
			- 收集数据
			- 文本文件解析成为词条向量
			- 检查词条确保解析的正确性
			- 训练算法
			- 测试算法
			- 使用算法
		- 在代码中，导入spam(垃圾邮件)和ham(正常邮件)文件夹下的文本文件，将他们解析为词列表。案例中共有50封电子邮件，其中10封被随机选择为测试集。分类器所需要的概率计算只利用训练集中的文档完成。这种随机选择一部分作为训练集，剩余部分作为测试集的过程称为留存交叉验证。
	- 代码实现
		- 自行编写朴素贝叶斯算法
		- sklearn使用朴素贝叶斯算法
			- 在sklearn中有三种朴素贝叶斯。它们是GaussianNB，MultinomialNB和BernoulliNB。
				- GaussianNB就是先验为高斯分布的朴素贝叶斯。
				- MultinomialNB就是先验为多项式分布的朴素贝叶斯。
				- BernoulliNB就是先验为伯努利分布的朴素贝叶斯。
- 补充说明
	- 参考书为《Python3数据分析与机器学习实战》
	- 数据集和源码可以查看我的GitHub，欢迎star或者fork。