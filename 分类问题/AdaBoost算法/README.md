# AdaBoost算法
- 简介
	- 当一个分类器正确率不那么高时，称其为“弱分类器”，或者说该分类器的学习方法为“弱学习方法”。与之对应的，存在“强分类器”和“强学习方法”。强学习方法的正确率很高。
	- AdaBoost是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器（弱分类器），然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）。
	- AdaBoost是Adaptive Boosting（自适应)的缩写，它的自适应在于：前一个基本分类器分错的样本会得到加强，加权后的样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，知道达到某个预定的足够小的错误率，或者达到预先设定的最大迭代次数。
	- 常见的机器学习算法都可以建立弱分类器，不过最经常使用的弱分类器是单层决策树，又叫决策树桩（Decision Stump），即层数为1的决策树。
- 原理
	- 这里使用单层决策树作为弱分类器。每一个训练数据都有一个权值系数，注意不是弱分类器的系数。建立最佳单层决策树的依据就是：每个训练数据在单层决策树中的分类结果乘以自己的权值系数后相加的“和”最小，即分类误差最小化。
	- 整体说来，AdaBoost迭代算法分为三步。
		- 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权重：$1/N$。
		- 训练弱分类器。具体训练过程中，如果某个样本点已经被准确分类，那么在构造下一个训练集中，它的权重就被降低；相反，如果某个样本点没有被准确分类，那么它的权重就得到提高。然后，权重更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代进行下去。
		- 将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。
	- 运行过程
		- 训练数据中的每一个样本，并赋予一开始相等的权重，这些权重构成了向量D。首先在训练数据上训练一个弱分类器并计算分类器的错误率，然后在同一数据集上再次训练弱分类器。在第二次训练分类器中，会再次重新调整每个样本的权重，其中第一次分类错误的权重会提高，第一次分类正确的权重会降低。AdaBoost根据每个弱分类器的错误率进行计算，为每个分类器都配了一个权重$\alpha$。
		- 错误率的定义为：$$ \varepsilon=\frac{未正确分类的样本数目}{所有样本数目} $$
		- 而$\alpha$的计算公式为$$ \alpha=\frac{1}{2}\left(\frac{1-\varepsilon}{\varepsilon}\right) $$
		- 计算出$\alpha$之后，可以对权重向量D进行更新，D的计算方法如下。
		- 如果某个样本被错误分类，那么该样本的权重更改为：$$ D_i^{(t+1)}=\frac{D_i^{(t)}e^\alpha}{sum(D)} $$
		- 如果某个样本被正确分类，那么该样本的权重修改为:$$ D_i^{(t+1)}=\frac{D_i^{(t)}e^{-\alpha}}{sum(D)} $$
		- 计算出D后，AdaBoost又开始下一轮迭代。AdaBoost算法会不断地重复训练和调整权重，直到训练错误率为0，或者弱分类器的数目达到用户的指定值。
- 实战
	- 基于单层决策树构建分类算法
	- 说明
		- 构造了一个简单数据集。
		- 单层决策树的一个著名问题就是难以通过一个分类器直接区分数据集。所以使用多个单层决策树，就可以构建一个能够解决分类的分类器。
	- 流程
		- 利用buildStump函数找到最佳的单层决策树
		- 将最佳单层决策树加入单层决策树数组
		- 计算$\alpha$
		- 计算新的权重向量D
		- 更新累计类别估计值
		- 如果错误率等于0.0，则退出循环
	- 代码实现
		- 如下
- 补充说明
	- 考书《Python3数据分析与机器学习实战》
	- 具体数据集和代码可以查看我的GitHub,欢迎star或者fork