# 支持向量机（SVM）
- 简介
	- 支持向量机（Support Vector Machine，SVM），是常见的一种判别方法。
	- 在机器学习领域，是一个监督学习模型，通常用来进行模式识别、分类及回归分析。
	- 与其他算法相比，支持向量机在学习复杂的非线性方程时提供了一种更为清晰、更加强大的方式。
	- 支持向量机是20世纪90年代中期发展起来的基于统计学习理论的一种机器学习方法，通过寻求结构化风险最小来提高学习机泛化能力，实现经验风险和置信范围的最小化，从而达到在统计样本量较少的情况下，也能获得良好统计规律的目的。
	- 通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，即支持向量机的学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。
- 原理
	- 几个概念
		- 线性可分
			- 如图，数据之间分隔得足够开，很容易在图中画出一条直线将这一组数据分开，这组数据就被称为线性可分数据。
			- ![](https://img-blog.csdnimg.cn/20190125122836810.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pob3VjaGVuMTk5OA==,size_16,color_FFFFFF,t_70)
		- 分隔超平面
			- 上述将数据集分隔开来的直线称为分隔超平面，由于上面给出的数据点都在二维平面上，所以此时的分隔超平面是一条直线。如果给出的数据集点是三维的，那么用来分隔数据的就是一个平面。因此，更高维的情况可以以此类推，如果数据是100维的，那么就需要一个99维的对象来对数据进行分隔，这些统称为超平面。
		- 间隔
			- 如图片所示，这条分隔线的好坏如何呢？我们希望找到离分隔超平面最近的点，确保它们离分割面的距离尽可能远。在这里点到分隔面的距离称为间隔。间隔尽可能大是因为如果犯错或在有限数据上训练分类器，我们希望分类器尽可能健壮。
		- 支持向量
			- 离分隔超平面最近的那些点是支持向量。
	- 求解
		- 接下来就是要求解最大支持向量到分隔面的距离，需要找到此类问题的求解方法。如图，分隔超平面可以写成$w^Tx+b$。要计算距离，值为$\frac{|w^TA+b|}{||w||}$。这里的向量w和常数b一起描述了所给数据的超平面。
		- 对$w^Tx+b$使用单位阶跃函数得到$f(w^Tx+b)$,其中当$u<0$时，$f(u)$输出-1，反之则输出+1。这里使用-1和+1是为了教学上的方便处理，可以通过一个统一公式来表示间隔或数据点到分隔超平面的距离。间隔通过$label\times{|w^Tx+b| \over ||w||}$来计算。如果数据处在正方向（+1）类里面且离分隔超平面很远的位置时，$w^Tx+b$是一个很大的正数。同时$label\times{|w^Tx+b| \over ||w||}$也会是一个很大的正数。如果数据点处在负方向（-1）类且离分隔超平面很远的位置时，由于此时类别标签为-1，$label\times{\frac{|w^Tx+b|}{||w||}}$仍然是一个很大的正数。
		- 为了找到具有最小间隔的数据点，就要找到分类器中定义的$w$和$b$，最小间隔的数据点也就是支持向量。一旦找到支持向量，就需要对该间隔最大化，可以写作$$ \max_{w,b}\left(\min_n(label\times{|w^Tx+b| \over ||w||})\right) $$
		- 但是直接求解上述问题是非常困难的，所以这里引入拉格朗日乘子法，可以把表达式写成下面的式子：
		- $$ {\max\_{\alpha}}\left[\sum\_{i=1}^m\alpha \frac{1}{2}\sum\_{i,j=1}^mlabel^{(i)}\times \alpha\_i\times\alpha\_j{(x^{(i)},x^{(j)})} \right] $$
		- 其中表示，两个向量的内积。
		- 约束条件为$$ C\geq\alpha\geq0 $$ $$ \sum_{i-1}^m \alpha\_i\times label^{(i)}=0 $$
		- 这里的常数C用于控制最大化间隔和保证大部分点的函数间隔小于1.0这两个目标的权重。因为所有数据都可能有干扰数据，所以通过引入所谓的松弛变量，允许有些数据点可以处于分隔面错误的一侧。
		- 根据上式子可知，只要求出所有的 $\alpha$，那么分隔面就可以通过$\alpha$来表达，SVM的主要工作就是求$\alpha$。
		- 这样一步步解出分隔面，那么分类问题游刃而解。
- 优缺点
	- 优点
		- 泛化错误率低，具有良好的学习能力。
		- 几乎所有分类问题都可以使用SVM解决。
		- 节省内存开销。
- 实战
	- 实现手写识别系统
	- 说明
		- 为了简单起见，这里的手写识别只针对0到9的数字，为了方便，图像转为了文本。目录trainingDigits中含有2000个例子，用于训练，目录testDigits含有900个例子，用于测试。
		- 虽然手写识别可以用KNN实现而且效果不错，但是KNN毕竟太占内存了，而且要保证性能不变的同时使用较少的内存。而对于SVM，只需要保留很少的支持向量就可以实现目标效果。
	- 流程
		- 准备数据
		- 分析数据
		- 使用SMO算法求出$\alpha$和$b$
		- 训练算法
		- 测试算法
		- 使用算法
	- 代码实现
		- 如下
- 补充说明
	- 参考书《Python3数据分析与机器学习实战》
	- 具体数据集和代码可以查看我的GitHub,欢迎star或者fork
	