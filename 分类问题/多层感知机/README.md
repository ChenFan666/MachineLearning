# 多层感知机（Multi-Layer Perceptron）
- 简介
	- 生物神经网络具有相互连接的神经元，神经元带有接受输入信号的树突，然后基于这些输入，它们通过轴突向另一个神经元产生输出信号。使用人工神经网络（ANN）来模拟这个过程，称为神经网络。神经网络是一个试图模仿自然生物神经网络的学习模式的机器学习框架。创建神经网络的过程从最基本的形式单个感知器开始。
	- 感知器就是一个能够把训练集的正例和反例划分为两个部分的器件，并且能够对未来输入的数据进行分类。举个例子：一个小孩子的妈妈在教他什么是苹果什么不是，首先会去拿一个苹果过来说“这个是苹果”，然后拿一个水杯过来说“这不是”，然后拿一个稍微不一样的苹果过来说“这也是”，，，最后，小孩子学习到了用一个模型（判断苹果的标准）来判断什么是苹果什么不是。
	- 感知器具有一个或者多个输入、偏置、激活函数和单个输出。感知器接受输入，将它们乘以一些权重，然后将它们传递到激活函数以产生输出。有许多激活函数可供选择，如逻辑函数、三角函数、阶跃函数等。我们还确保向感知器添加偏差，这避免了所有输入可能等于零的问题（这意味着没有乘以权重会有影响）。
- 原理
	- 感知器的输入空间(特征空间）一般为$R^n\supseteq X$，即n维向量空间，输出空间为$Y=\{+1,-1\}$即$-1$代表反例，$+1$代表正例。例如，输入$x\in X$，对应于输入空间的$R^n$中的某个点，而输出$y\in Y$表示改点所在的分类。需要注意的是，输入$x$是一个n维的向量，即$x=(x_1,x_2,...x_n)$。现在，已经有了输入和输出的定义，就可以给出感知机$f(x)$的模型：$$f(x)=sign(w\times x+b)$$其中，向量$x=(x_1,x_2,...x_n)$中的每个分量代表输入向量空间$R^n$中向量$x$的每个分量$x_i$的权重，或者说参数。$b \in R$称为偏差，$w \times x$表示向量$w$和$x$的内积，$sign$是一个符号函数，即$$sign(x) =\begin{cases} +1 \ x\ge 0\\ -1 \ x<0 \end{cases}$$上面这个函数$f(x)$称为感知机。
- 神经网络简述
	- 神经网络一般由输入层（Input Layer）、隐藏层（Hidden Layer）、输出层（Output Layer）组成，每层由单元（Units）组成，输入层是由训练集的实例特征向量传入，经过连接节点的权重（Weight）传入下一层，上一层的输出是下一层的输入，隐藏层的个数是任意的，输出层和输入层只有一个。如图。
		- ![来源网络，侵删](https://img-blog.csdnimg.cn/20190301183740839.png)
	- 当有输出的时候，可以将其与已知标签进行比较，并相应地调整权重（权重通常以随机初始化值开始）。重复此过程，直到达到允许迭代的最大数量或可接受的错误率。上面网络中如果每个神经元都是由感知器构成的，则这个网络称为Multi-layer Perceptron多层感知机。多层感知机的优点是：可以学习非线性模型，并且可以实时学习；然而，多层感知机也有自身的缺点：有隐藏层的MLP包含一个非凸性损失函数，存在超过一个最小值，所以不同的随机初始权重可能导致不同的验证精确度；MLP要求调整一系列超参数，如隐藏神经元、隐藏层的个数以及迭代的次数；MLP对特征缩放比较敏感。
	- 在使用神经网络前需要注意以下事项。
		- 1.使用神经网络训练数据之前，必须确定神经网络层数，以及每层单元个数。
		- 2.特征向量在被传入输入层时通常被先标准化（Normalize）到0和1之间。
		- 3.离散型变量可以被编码成每一个输入单元对应一个特征可能赋的值。例如，特征值A可能取三个值（$a_0,a_1,a_2$），可以使用3个输入单元来代表A，如果$A=a_0$，那么代表$a_0$的单元值就取1，其他取0；如果$A=a_1$，那么代表$a_1$的单元值就取1，其他取0，以此类推。
		- 4.神经网络既可以用来做分类（Classification）问题，也可以解决回归（Regression）问题。对于分类问题，如果是两类，可以用一个输入单元表示（0和1分别代表两类），如果多于两类，每一个类别用一个输入单元表示，所以输入层的单元数量通常等于类别的数量，没有明确的规则来设计最好有多少个隐藏层，可以根据实验测试和误差，以及准确度来实验并改进最优的隐藏层数目。
- 实战
	- 使用多层感知器分析，根据葡萄酒的各项化学特征来判断葡萄酒的优劣
	- 使用sklearn封装的MLPClassifier
		- 在代码中设定的参数solver='lbfgs'为求解方法设置，一般有3个取值。
			- lbfgs:使用quasi-Newton方法的优化器。（小数据集更好，收敛快，效果好）
			- sgd:使用随机梯度下降。
			- adam:使用Kingma、DiederikheJimmy Ba提出的机遇随机梯度的优化器。（大数据集表现好）
		- 参数alpha是L2的参数，MLP支持正则化，默认L2.
		- 参数hidden_layer_sizes=(5,2)表示隐藏层有两层，第一层5个神经元，第二层2个神经元。
	- 代码实现
		- 如下
- 补充说明
	- 考书《Python3数据分析与机器学习实战》
	- 具体数据集和代码可以查看我的GitHub,欢迎star或者fork