# 朴素贝叶斯
- 地位
 - 著名的有监督学习算法，主要用来分类，在一些情况下可以和决策树以及神经网络媲美。但是由于基于条件特征独立性和连续变量的正态性假设为前提，精度有所问题。
- 原理
 - 朴素贝叶斯是贝叶斯决策论的一部分
   - 简单介绍这个著名的决策论
     - 我们使用P1(x)表示一个数据属于类别1的概率，使用P2(x)表示一个数据属于类别2的概率，现在有一个新数据x0且有P1(x0)>P2(x0)那么就说此时这个数据属于类别1。
     - 这是一个很通俗的解释，可是这就是贝叶斯决策论的核心。
 - 在能够进行朴素贝叶斯学习之前我希望你首先要有足够的概率论基础（这是大学的必修课），至少要知道如何计算条件概率，全概率。（这些知识相当基础而简单）
   - 条件概率公式
   - ![条件概率公式](http://g.recordit.co/18sWqnko9b.gif)
   - 对这个公式进行变形。
   - ![贝叶斯变形公式、](http://g.recordit.co/JPuonrHFGp.gif)
   - 这看起来没有什么区别，但是提出来了几个概念。
  	 - P(A)称为“先验概率”（Prior probability），即在B事件发生之前，对A事件概率进行判断。
  	 - P(A|B)称为“后验概率”（Posterior probability），即在B事件发生之后，对A事件概率的重新判断。
  	 - P(B|A)/P(B)称为“可能性函数”（Likelyhood），这是一个调整因子，使得预估概率更接近真实概率。
  	 - 也就是说 后验概率 = 先验概率 * 调整因子
  	 - 这就是贝叶斯推断，其含义就是先估计一个“先验概率”，然后加入实验数据，观察这个数据是加强还是减弱了“先验概率”，从而得到更加接近事实的“后验概率”。
  	 - 若调整因子大于1表示加强“先验概率”，反之减弱，等于1表示没有影响。
  - 朴素贝叶斯则和贝叶斯推断有所不同，它认为条件概率分布的条件是独立不相关的。（在概率论里就是可不可拆分概率的区别）
      - 这就带来了一些问题。首先，既然计算方便，那么概率连乘就被允许了，然而这是有很大的风险的，因为只要一个概率为0，那么结果毋庸置疑就是0了，这里可以使用拉普拉斯平滑处理（加1平滑）。其次，小数相乘只会越来越小，在近似运算后可能出现不可预计的后果（如近似得0风险），这就是下溢出问题，解决方法一般是取对数。
- 朴素贝叶斯优点
 - 它是生成式模型，通过计算概率来进行分类，所以它可以用来处理多分类问题。
 - 算法简单易懂，数据少时表现好。
- 朴素贝叶斯缺点
 - 对输入数据格式要求高。
 - 准确率丢失。（由于朴素假设）
 - 需要计算先验概率，分类决策存在错误率。
 - 朴素贝叶斯的准确率，是比较依赖于训练集的，其实不只是它，很多机器学习的算法的可能性都取决于训练条件。
- 为了了解这个算法的实现和参数，建议自己编写一次朴素贝叶斯，由于代码繁琐，这里不列出了。
- 这里只列出sklearn如何使用朴素贝叶斯。
  - 在sklearn中有三种朴素贝叶斯。它们是GaussianNB，MultinomialNB和BernoulliNB。
	 - GaussianNB就是先验为高斯分布的朴素贝叶斯。
	 - MultinomialNB就是先验为多项式分布的朴素贝叶斯。
	 - BernoulliNB就是先验为伯努利分布的朴素贝叶斯。