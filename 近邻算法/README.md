# k近邻算法（KNN）
- 简介：这是最基础的单标签分类算法之一。
- 原理：给定一个样本数据集（也称为训练集，其实这两者是不同的，这里暂时可以这样认为两者等同），样本集内每一个数据都已经存在标签。（例如：'兔子'这个数据的标签是'动物'）。现在，输入一个新的数据，含有诸多属性，将每一个属性与样本集中的数据对应的属性进行比较，通过算法找到样本集中与它最相似的k个数据，
再获得这k个样本数据出现最多的标签作为这个新数据的标签。
- 例如

| 数据编号| 腿的个数 | 翅膀个数 | 尾巴个数 | 标签 |
| :---:|:---:| :---:| :---: | :---: |
| data1 | 4 | 0 | 0 | A |
| data2 | 2 | 1 | 1 | B |
| data3 | 0 | 0 | 0 | C |
- 这时，一个新的数据需要分类。
 
| 数据编号| 腿的个数 | 翅膀个数 | 尾巴个数 |
| :---:|:---:| :---:| :---: |
| datainput | 4 | 0 | 0 |
- 我们将待标记的数据和样本集里面的数据比对，发现它的每一个属性均和data1一致，也只有data1和它完全一致，不难给与标签为A。
- 在上面这个看似简单的过程中其实有很多值得思考的东西。
- 
    - 我是如何判断data1和它一致呢？
    - 我是相同属性去比较的，然后发现每一个属性值完全相同。可是这是因为属性少，数据量少，而且一个“等与不等”就能给出结论。但是事实上很多时候（几乎所有时候）完全一致的两个数据是几乎不存在的，那种情况下那么多的属性，我如何度量两个数据的接近程度，如何得到最接近的k个数据呢？
    - 这个问题其实不难回答，只要求出待标记数据与已知数据的属性距离就ok了，至于距离怎么求？这个也不难，如果只有两个属性，可以建立二维坐标系，求出新数据的点与其他所有点的距离即可。如果属性更多，可以从二维上推广，得到高维两点之间的距离公式。
    
![公式1](http://g.recordit.co/o8vwskM02E.gif)
 - 这个公式其实就是欧几里得度量公式。（也可以使用曼哈顿距离公式，当然这两个都是闵可夫斯基距离公式的特例,但是只有欧式距离保证收敛）
 - 就这样，我们计算出了待标记数据和所有样本数据的距离，这是个数值，比较得到最接近的k个数据，统计这k个数据的标签频率，选取出现频率最高的或者最高的中的一个，为新数据打上标签。
 - 代码如文件所示。